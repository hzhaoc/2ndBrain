# Recurrent Neural Network
- [SunLab](http://sunlab.org/teaching/cse6250/spring2020/dl/dl-rnn.html#recurrent-neural-networks-2)
- [Colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

# Implementation
- [Pytorch RNN example](https://github.com/ast0414/CSE6250BDH-LAB-DL/blob/master/3_RNN.ipynb)
- When use `dropout` param in RNN with like GRU layer or LSTM layer, it may lead to better validation/testing performance than training, explained [here](https://stackoverflow.com/questions/43979449/higher-validation-accuracy-than-training-accurracy-using-tensorflow-and-keras)
- [Why use pack sequence in rnn forward?](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch)
- [select last sequence in forward](https://discuss.pytorch.org/t/selecting-element-on-dimension-from-list-of-indexes/36319)